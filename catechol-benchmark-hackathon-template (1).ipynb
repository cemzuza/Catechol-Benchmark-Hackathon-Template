{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport os, random\nimport numpy as np\nimport pandas as pd\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nimport torch\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T18:08:24.115539Z","iopub.execute_input":"2026-01-16T18:08:24.115812Z","iopub.status.idle":"2026-01-16T18:08:31.402934Z","shell.execute_reply.started":"2026-01-16T18:08:24.115793Z","shell.execute_reply":"2026-01-16T18:08:31.402198Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/catechol-benchmark-hackathon/drfps_catechol_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/catechol_full_data_yields.csv\n/kaggle/input/catechol-benchmark-hackathon/catechol_single_solvent_yields.csv\n/kaggle/input/catechol-benchmark-hackathon/fragprints_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/acs_pca_descriptors_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/utils.py\n/kaggle/input/catechol-benchmark-hackathon/spange_descriptors_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/smiles_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/drfps_catechol_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/catechol_full_data_yields.csv\n/kaggle/input/catechol-benchmark-hackathon/catechol_single_solvent_yields.csv\n/kaggle/input/catechol-benchmark-hackathon/fragprints_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/acs_pca_descriptors_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/utils.py\n/kaggle/input/catechol-benchmark-hackathon/spange_descriptors_lookup.csv\n/kaggle/input/catechol-benchmark-hackathon/smiles_lookup.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\n\nfrom utils import INPUT_LABELS_FULL_SOLVENT, INPUT_LABELS_SINGLE_SOLVENT, INPUT_LABELS_NUMERIC, INPUT_LABELS_SINGLE_FEATURES, INPUT_LABELS_FULL_FEATURES, load_data, load_features, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T18:08:31.404048Z","iopub.execute_input":"2026-01-16T18:08:31.404352Z","iopub.status.idle":"2026-01-16T18:08:31.417896Z","shell.execute_reply.started":"2026-01-16T18:08:31.404333Z","shell.execute_reply":"2026-01-16T18:08:31.417316Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from abc import ABC, abstractmethod\n\nclass SmilesFeaturizer(ABC):\n    @abstractmethod\n    def featurize(self, X):\n        \"\"\"Return torch tensor features from dataframe X.\"\"\"\n        ...\n\nclass BaseModel(ABC):\n    @abstractmethod\n    def train_model(self, X_train, y_train):\n        ...\n\n    @abstractmethod\n    def predict(self, X):\n        ...\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T18:08:33.205418Z","iopub.execute_input":"2026-01-16T18:08:33.205716Z","iopub.status.idle":"2026-01-16T18:08:33.210394Z","shell.execute_reply.started":"2026-01-16T18:08:33.205693Z","shell.execute_reply":"2026-01-16T18:08:33.209646Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntorch.set_default_dtype(torch.float32)\n\n# ----------------------------\n# helpers\n# ----------------------------\ndef _strip_cols(df):\n    return df.rename(columns=lambda c: c.strip())\n\ndef _clean_names(s):\n    return pd.Series(s).astype(str).str.strip()\n\ndef _find_single_solvent_col(X):\n    Xc = _strip_cols(X)\n    if \"SOLVENT NAME\" in Xc.columns:\n        return \"SOLVENT NAME\"\n    if \"SOLVENT\" in Xc.columns:\n        return \"SOLVENT\"\n    for c in Xc.columns:\n        if \"SOLVENT\" in c.upper():\n            return c\n    raise KeyError(\"Cannot find solvent column in single task\")\n\nclass _TorchScaler:\n    def __init__(self):\n        self.mean = None\n        self.std = None\n\n    def fit(self, X):\n        self.mean = X.mean(dim=0, keepdim=True)\n        self.std = X.std(dim=0, keepdim=True)\n        self.std = torch.where(self.std < 1e-12, torch.ones_like(self.std), self.std)\n\n    def transform(self, X):\n        return (X - self.mean.to(X.device)) / self.std.to(X.device)\n\n    def inverse(self, X):\n        return X * self.std.to(X.device) + self.mean.to(X.device)\n\n# ----------------------------\n# numeric features (BASELINE)\n# ----------------------------\ndef _num_single(rt, T):\n    return np.concatenate(\n        [rt, T, np.log1p(rt), T**2, rt*T],\n        axis=1\n    )  # 5\n\ndef _num_full(rt, T):\n    Tk = T + 273.15\n    invT = 1.0 / np.clip(Tk, 1e-6, None)\n    base = np.concatenate([rt, T, np.log1p(rt), T**2, rt*T], axis=1)\n    kin = np.concatenate([invT, rt*invT], axis=1)\n    return np.concatenate([base, kin], axis=1)  # 7\n\n# ----------------------------\n# Featurizers\n# ----------------------------\nclass FeaturizerSingle:\n    def __init__(self, features=(\"spange_descriptors\", \"acs_pca_descriptors\")):\n        feats = [load_features(f) for f in features]\n        self.feats = pd.concat(feats, axis=1)\n        self.dim = 5 + self.feats.shape[1]\n\n    def featurize(self, X):\n        X = _strip_cols(X)\n        sol_col = _find_single_solvent_col(X)\n\n        rt = X[\"Residence Time\"].to_numpy(np.float32).reshape(-1, 1)\n        T  = X[\"Temperature\"].to_numpy(np.float32).reshape(-1, 1)\n        num = _num_single(rt, T)\n\n        sol = _clean_names(X[sol_col]).values\n        sol_feat = self.feats.loc[sol].to_numpy(np.float32)\n\n        return torch.tensor(np.concatenate([num, sol_feat], axis=1))\n\nclass FeaturizerFull:\n    def __init__(self, features=(\"spange_descriptors\", \"acs_pca_descriptors\")):\n        feats = [load_features(f) for f in features]\n        self.feats = pd.concat(feats, axis=1)\n        d = self.feats.shape[1]\n        self.dim = (7 + 2) + 5 * d\n\n    def featurize(self, X):\n        X = _strip_cols(X)\n\n        rt = X[\"Residence Time\"].to_numpy(np.float32).reshape(-1, 1)\n        T  = X[\"Temperature\"].to_numpy(np.float32).reshape(-1, 1)\n        frac = X[\"SolventB%\"].to_numpy(np.float32).reshape(-1, 1)\n\n        num = _num_full(rt, T)\n        mix = np.concatenate([frac, frac*(1-frac)], axis=1)\n\n        A = _clean_names(X[\"SOLVENT A NAME\"]).values\n        B = _clean_names(X[\"SOLVENT B NAME\"]).values\n\n        featA = self.feats.loc[A].to_numpy(np.float32)\n        featB = self.feats.loc[B].to_numpy(np.float32)\n\n        avg = featA*(1-frac) + featB*frac\n        diff = featB - featA\n        inter = diff * frac\n\n        return torch.tensor(\n            np.concatenate([num, mix, featA, featB, avg, diff, inter], axis=1)\n        )\n\n# ----------------------------\n# Network\n# ----------------------------\nclass _MLP(nn.Module):\n    def __init__(self, in_dim, hidden=(256,256,128), dropout=0.10):\n        super().__init__()\n        layers = []\n        prev = in_dim\n        for h in hidden:\n            layers += [\n                nn.Linear(prev, h),\n                nn.BatchNorm1d(h),\n                nn.SiLU(),\n                nn.Dropout(dropout),\n            ]\n            prev = h\n        layers.append(nn.Linear(prev, 3))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n# ----------------------------\n# Model\n# ----------------------------\nclass MLPModel(nn.Module, BaseModel):\n    def __init__(\n        self,\n        data=\"single\",\n        base_seed=41,\n        lr=1.2e-3,\n        weight_decay=1e-3,\n        epochs=450,\n        batch_size=256,\n    ):\n        super().__init__()\n        self.data = data\n        self.base_seed = base_seed\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.epochs = epochs\n        self.batch_size = batch_size\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.featurizer = FeaturizerSingle() if data==\"single\" else FeaturizerFull()\n        self.net = _MLP(self.featurizer.dim).to(self.device)\n\n        self.x_scaler = _TorchScaler()\n        self.y_scaler = _TorchScaler()\n\n    def _seed(self):\n        np.random.seed(self.base_seed)\n        torch.manual_seed(self.base_seed)\n\n    def train_model(self, X, y):\n        self._seed()\n\n        Xt = self.featurizer.featurize(X)\n        yt = torch.tensor(y.values, dtype=torch.float32)\n\n        self.x_scaler.fit(Xt)\n        self.y_scaler.fit(yt)\n\n        Xt = self.x_scaler.transform(Xt)\n        yt = self.y_scaler.transform(yt)\n\n        ds = TensorDataset(Xt, yt)\n        dl = DataLoader(ds, batch_size=self.batch_size, shuffle=True)\n\n        opt = torch.optim.AdamW(self.net.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs)\n        loss_fn = nn.MSELoss()\n\n        self.net.train()\n        for _ in range(self.epochs):\n            for xb, yb in dl:\n                xb = xb.to(self.device)\n                yb = yb.to(self.device)\n                opt.zero_grad()\n                loss = loss_fn(self.net(xb), yb)\n                loss.backward()\n                nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)\n                opt.step()\n            sched.step()\n\n    @torch.no_grad()\n    def predict(self, X):\n        self.net.eval()\n        Xt = self.x_scaler.transform(self.featurizer.featurize(X)).to(self.device)\n        y = self.y_scaler.inverse(self.net(Xt))\n        return torch.clamp(y, 0.0, 1.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T18:08:43.496938Z","iopub.execute_input":"2026-01-16T18:08:43.497883Z","iopub.status.idle":"2026-01-16T18:08:43.523460Z","shell.execute_reply.started":"2026-01-16T18:08:43.497856Z","shell.execute_reply":"2026-01-16T18:08:43.522726Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nimport tqdm\n\nX, Y = load_data(\"single_solvent\")\n\nsplit_generator = generate_leave_one_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = MLPModel(\n    data=\"single\",\n    base_seed=41 + fold_idx\n    )\n\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  \n\n    \n    predictions_np = predictions.detach().cpu().numpy()\n\n\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 0,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n\nsubmission_single_solvent = pd.DataFrame(all_predictions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T18:12:08.779849Z","iopub.execute_input":"2026-01-16T18:12:08.780482Z","iopub.status.idle":"2026-01-16T18:14:21.554409Z","shell.execute_reply.started":"2026-01-16T18:12:08.780459Z","shell.execute_reply":"2026-01-16T18:14:21.553781Z"}},"outputs":[{"name":"stderr","text":"24it [02:12,  5.53s/it]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\nX, Y = load_data(\"full\")\n\nsplit_generator = generate_leave_one_ramp_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = MLPModel(\n    data=\"full\",\n    base_seed=1041 + fold_idx\n    )\n\n \n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  \n\n    \n    predictions_np = predictions.detach().cpu().numpy()\n\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 1,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n\nsubmission_full_data = pd.DataFrame(all_predictions)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T18:24:06.247774Z","iopub.execute_input":"2026-01-16T18:24:06.248530Z","iopub.status.idle":"2026-01-16T18:26:04.130158Z","shell.execute_reply.started":"2026-01-16T18:24:06.248500Z","shell.execute_reply":"2026-01-16T18:26:04.129334Z"}},"outputs":[{"name":"stderr","text":"13it [01:57,  9.06s/it]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\nsubmission = pd.concat([submission_single_solvent, submission_full_data])\nsubmission = submission.reset_index()\nsubmission.index.name = \"id\"\nsubmission.to_csv(\"submission.csv\", index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T18:28:20.115495Z","iopub.execute_input":"2026-01-16T18:28:20.116201Z","iopub.status.idle":"2026-01-16T18:28:20.139006Z","shell.execute_reply.started":"2026-01-16T18:28:20.116175Z","shell.execute_reply":"2026-01-16T18:28:20.138331Z"}},"outputs":[],"execution_count":12}]}